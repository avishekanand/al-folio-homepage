<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2026-03-01T14:11:33+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Avishek Anand</title><subtitle>This is the homepage&gt;</subtitle><entry><title type="html">Adaptive Retrieval</title><link href="http://localhost:4000/blog/2025/adaptive-retrieval/" rel="alternate" type="text/html" title="Adaptive Retrieval" /><published>2025-01-12T13:00:00+01:00</published><updated>2025-01-12T13:00:00+01:00</updated><id>http://localhost:4000/blog/2025/adaptive-retrieval</id><content type="html" xml:base="http://localhost:4000/blog/2025/adaptive-retrieval/"><![CDATA[<h1 id="from-graphs-to-llms-our-journey-in-adaptive-retrieval">From Graphs to LLMs: Our Journey in Adaptive Retrieval</h1>

<p>When Sean MacAvaney introduced the idea of <strong>Adaptive Retrieval</strong> at ECIR 2024’s <em>ironGraphs</em> workshop, it immediately resonated with me. Sean had first formalized adaptive retrieval in 2022 [1], showing how re-rankers could dynamically expand their view of the corpus by exploiting document-to-document relationships. For me, this clicked with an open challenge I’d been discussing with my PhD student, Mandeep, on the intersection of graphs and retrieval. The fit was perfect: graph signals could be a natural substrate for adaptive re-ranking.</p>

<p>From that starting point, we began exploring how to push adaptive retrieval further. Over the last few years, this work has crystallized into a series of papers: <strong>QUAM (WSDM)</strong>[2], <strong>SlideGAR (ECIR)</strong>[3], <strong>ORE (SIGIR)</strong>[4], and most recently, <strong>SUNAR</strong>[5]. Each represents a step in broadening the scope of adaptive retrieval—from graph-based reranking, to LLM-based re-rankers, to online estimation, and finally to incorporating LLM feedback itself.</p>

<h2 id="quam-query-affinity-modelling-for-adaptive-retrieval">QUAM: Query Affinity Modelling for Adaptive Retrieval</h2>

<p>Our first step was <strong>QUAM</strong> (<em>Query Affinity Modelling for Adaptive Retrieval</em>). While GAR (Graph-based Adaptive Retrieval) showed the potential of corpus graphs, it treated all neighborhoods the same. In QUAM, we introduced a more nuanced view: instead of blindly alternating between initial retrieval results and graph neighbors, we prioritized neighbors based on their <em>query affinity</em>.</p>

<p>This led to substantial improvements in recall without excessive overhead, showing that adaptive retrieval could be made both smarter and more efficient.</p>

<h2 id="slidegar-adaptive-retrieval-meets-llm-re-rankers">SlideGAR: Adaptive Retrieval Meets LLM Re-rankers</h2>

<p>As large language models emerged as powerful <strong>listwise re-rankers</strong>, we faced a new challenge. Adaptive retrieval methods like GAR and QUAM assumed pointwise rankers that output independent scores. LLM re-rankers, however, work on batches of documents at once, producing only an ordering.</p>

<p>In <strong>SlideGAR</strong>, we introduced a <strong>sliding window strategy</strong> that fed batches of documents to an LLM ranker, while still pulling in new candidates from the corpus graph. This bridged the gap between adaptive retrieval and listwise LLM ranking.</p>

<h2 id="ore-online-relevance-estimation-beyond-graphs">ORE: Online Relevance Estimation Beyond Graphs</h2>

<p>Our next step was <strong>ORE (Online Relevance Estimation)</strong>, which generalized adaptive retrieval into a broader <strong>bandit-based framework</strong>. Instead of committing to a fixed top-_k_ pool (as in telescoping pipelines), ORE continuously updated relevance estimates for documents during reranking. This allowed us to dynamically reprioritize documents—even those initially overlooked.</p>

<p>ORE achieved remarkable recall gains, outperforming both telescoping pipelines and earlier adaptive retrieval methods. It showed that adaptive retrieval could be reframed as <em>online estimation</em>, bridging hybrid, graph-based, and LLM-driven search.</p>

<h2 id="sunar-llm-feedback-for-complex-qa">SUNAR: LLM Feedback for Complex QA</h2>

<p>Most recently, we introduced <strong>SUNAR (Semantic Uncertainty-based Neighborhood Aware Retrieval)</strong> for <strong>complex QA</strong>. SUNAR brings a new dimension: it uses <strong>LLM feedback</strong> to guide retrieval. Specifically, it leverages the <em>uncertainty</em> of interim LLM-generated answers to promote or penalize candidate documents in a neighborhood-aware retrieval process.</p>

<p>The insight is simple but powerful: if the LLM shows uncertainty or inconsistency in its answers, retrieval should adapt by exploring alternative evidence. SUNAR achieved up to <strong>31.8% performance improvements</strong> on complex QA datasets like MusiqueQA and 2WikiMultiHopQA, substantially outperforming strong retrieve-and-reason baselines.</p>

<h2 id="looking-ahead">Looking Ahead</h2>

<p>What began as a question—<em>can re-ranker based retrieval be improved by exploiting graph structure?</em>—has evolved into a much broader research direction. We now see re-rankers not just as the final layer of search, but as <strong>active agents</strong> that integrate signals from graphs, hybrid retrievers, bandit-style estimation, and even <strong>LLM feedback</strong>.
Graph-based signals remain important, but they are just one piece of a larger picture. The future lies in <strong>feedback-driven adaptive retrieval</strong>: building systems where re-rankers and LLMs iteratively inform each other, reducing uncertainty, improving recall, and delivering more trustworthy answers.</p>

<p>With QUAM, SlideGAR, ORE, and SUNAR, we’ve traced a path from graph-enhanced reranking to feedback-driven retrieval. And we believe this is only the beginning.</p>

<hr />

<h2 id="references">References</h2>

<p>[1] Sean MacAvaney, Nicola Tonellotto, and Craig Macdonald. <em>Adaptive Re-Ranking with a Corpus Graph</em>. CIKM 2022. <a href="https://arxiv.org/pdf/2208.08942">paper</a></p>

<p>[2] Mandeep Rathee, Sean MacAvaney, and Avishek Anand. <em>QUAM: Adaptive Retrieval through Query Affinity Modelling</em>. WSDM 2024. <a href="https://dl.acm.org/doi/pdf/10.1145/3531681.3531706">paper</a></p>

<p>[3] Mandeep Rathee, Venktesh V, Sean MacAvaney, and Avishek Anand. <em>SlideGAR: Adaptive Retrieval for Listwise Re-Rankers</em>. ECIR 2024. <a href="https://arxiv.org/pdf/2501.09186">paper</a></p>

<p>[4] Mandeep Rathee, Venktesh V, Sean MacAvaney, and Avishek Anand. <em>Breaking the Lens of the Telescope: Online Relevance Estimation over Large Retrieval Sets</em>. SIGIR 2025. <a href="https://dl.acm.org/doi/pdf/10.1145/3726302.3729910">paper</a></p>

<p>[5] Venktesh V, Mandeep Rathee, and Avishek Anand. <em>SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex QA</em>. NAACL 2025. <a href="https://aclanthology.org/2025.naacl-long.300.pdf">paper</a></p>]]></content><author><name></name></author><category term="research" /><category term="retrieval" /><category term="machine-learning" /><category term="research" /><summary type="html"><![CDATA[A blog post about adaptive retrieval techniques]]></summary></entry></feed>